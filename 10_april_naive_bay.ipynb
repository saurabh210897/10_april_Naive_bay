{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d8432ba0-1ee2-4a78-b380-f7c7d021b70b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q1. A company conducted a survey of its employees and found that 70% of the employees use the \n",
    "# company's health insurance plan, while 40% of the employees who use the plan are smokers. What is the \n",
    "# probability that an employee is a smoker given that he/she uses the health insurance plan?\n",
    "\n",
    "# Q2. What is the difference between Bernoulli Naive Bayes and Multinomial Naive Bayes?\n",
    "\n",
    "# Q3. How does Bernoulli Naive Bayes handle missing values?\n",
    "\n",
    "# Q4. Can Gaussian Naive Bayes be used for multi-class classification?\n",
    "\n",
    "# Q5. Assignment:\n",
    "# Data preparation:\n",
    "\n",
    "# Download the \"Spambase Data Set\" from the UCI Machine Learning Repository (https://archive.ics.uci.edu/ml/\n",
    "# datasets/Spambase). This dataset contains email messages, where the goal is to predict whether a message \n",
    "# is spam or not based on several input features.\n",
    "\n",
    "\n",
    "# Implementation:\n",
    "\n",
    "# Implement Bernoulli Naive Bayes, Multinomial Naive Bayes, and Gaussian Naive Bayes classifiers using the \n",
    "# scikit-learn library in Python. Use 10-fold cross-validation to evaluate the performance of each classifier on the \n",
    "# dataset. You should use the default hyperparameters for each classifier.\n",
    "\n",
    "\n",
    "# Results:\n",
    "\n",
    "# Report the following performance metrics for each classifier:\n",
    "\n",
    "# Accuracy\n",
    "\n",
    "# Precision\n",
    "\n",
    "# Recall\n",
    "\n",
    "# F1 score\n",
    "\n",
    "\n",
    "# Discussion:\n",
    "\n",
    "# Discuss the results you obtained. Which variant of Naive Bayes performed the best? Why do you think that is \n",
    "# the case? Are there any limitations of Naive Bayes that you observed?\n",
    "\n",
    "\n",
    "# Conclusion:\n",
    "\n",
    "# Summarise your findings and provide some suggestions for future work\n",
    "\n",
    "# Note:  This dataset contains a binary classification problem with multiple features. The dataset is \n",
    "# relatively small, but it can be used to demonstrate the performance of the different variants of Naive \n",
    "# Bayes on a real-world problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b7c09ca9-dc54-4ff7-9251-769fd6e088bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q1. A company conducted a survey of its employees and found that 70% of the employees use the \n",
    "# company's health insurance plan, while 40% of the employees who use the plan are smokers. What is the \n",
    "# probability that an employee is a smoker given that he/she uses the health insurance plan?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d34c0775-a0f7-4343-87ce-65344a818caa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# To calculate the probability that an employee is a smoker given that they use the health insurance plan, we need to use Bayes' theorem:\n",
    "\n",
    "# P(Smoker|Uses health insurance plan) = P(Uses health insurance plan|Smoker) * P(Smoker) / P(Uses health insurance plan)\n",
    "\n",
    "# where:\n",
    "\n",
    "# P(Smoker|Uses health insurance plan) is the probability of an employee being a smoker given that they use the health insurance plan.\n",
    "# P(Uses health insurance plan|Smoker) is the probability of an employee using the health insurance plan given that they are a smoker, which is given as 40%.\n",
    "# P(Smoker) is the overall probability of an employee being a smoker, which is not given in the question.\n",
    "# P(Uses health insurance plan) is the overall probability of an employee using the health insurance plan, which is given as 70%.\n",
    "# To find P(Smoker), we can use the law of total probability, which states that the probability of an event can be found by considering all \n",
    "# the possible ways that it can occur:\n",
    "\n",
    "# P(Smoker) = P(Smoker|Uses health insurance plan) * P(Uses health insurance plan) + P(Smoker|Does not use health insurance plan) * \n",
    "# P(Does not use health insurance plan)\n",
    "\n",
    "# We are not given the probability of an employee being a smoker given that they do not use the health insurance plan, but we can assume that \n",
    "# it is lower than the probability of an employee being a smoker given that they do use the plan. Let's assume that P(Smoker|Does not use health insurance plan) is 20%.\n",
    "# Then, using the law of total probability, we can calculate P(Smoker) as follows:\n",
    "\n",
    "# P(Smoker) = P(Smoker|Uses health insurance plan) * P(Uses health insurance plan) + P(Smoker|Does not use health insurance plan) * \n",
    "# P(Does not use health insurance plan)\n",
    "# = 0.4 * 0.7 + 0.2 * 0.3\n",
    "# = 0.34\n",
    "\n",
    "# Now we can substitute the given values and the calculated values into Bayes' theorem:\n",
    "\n",
    "# P(Smoker|Uses health insurance plan) = P(Uses health insurance plan|Smoker) * P(Smoker) / P(Uses health insurance plan)\n",
    "# = 0.4 * 0.34 / 0.7\n",
    "# = 0.194\n",
    "\n",
    "# Therefore, the probability that an employee is a smoker given that they use the health insurance plan is 0.194, or about 19.4%."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1cd3c4a0-49fc-40a2-b049-3232c07818ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# probability of an employee using the health insurance plan\n",
    "p_uses_plan = 0.7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1308159a-63a5-4eef-8e87-54a5ae63ce78",
   "metadata": {},
   "outputs": [],
   "source": [
    "# probability of an employee being a smoker given that they use the health insurance plan\n",
    "p_plan_given_smoker = 0.4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "93a4dec7-99d0-435d-9b44-305890461f9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# probability of an employee being a smoker given that they do not use the health insurance plan\n",
    "p_no_plan_given_smoker = 0.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "43572212-c465-4970-93a6-df96628227c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# probability of an employee being a smoker\n",
    "p_smoker = p_plan_given_smoker * p_uses_plan + p_no_plan_given_smoker * (1 - p_uses_plan)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "fb6ef5c4-bbc8-428a-99ff-8ecfae2d8004",
   "metadata": {},
   "outputs": [],
   "source": [
    "# probability of an employee being a smoker given that they use the health insurance plan\n",
    "p_smoker_given_plan = p_plan_given_smoker * p_smoker / p_uses_plan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "fd0df376-ea22-49ce-bebd-5457458672dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.19428571428571428\n"
     ]
    }
   ],
   "source": [
    "print(p_smoker_given_plan) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7cf28f43-0efa-4876-8d28-58d46cf08efc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q2. What is the difference between Bernoulli Naive Bayes and Multinomial Naive Bayes?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e71661fe-54b5-4916-86a6-dbdfeb582335",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bernoulli Naive Bayes and Multinomial Naive Bayes are both algorithms used in machine learning for classification problems. \n",
    "# However, they differ in the type of data they can handle and the way they model the data.\n",
    "\n",
    "# Bernoulli Naive Bayes is used for binary classification problems where each feature is either present or absent. \n",
    "# For example, in a spam email classification problem, the presence or absence of certain keywords in the email can be used as features. \n",
    "# Bernoulli Naive Bayes models the data as a set of binary events, where each event represents the presence or absence of a feature. \n",
    "# It assumes that the features are conditionally independent given the class label.\n",
    "\n",
    "# Multinomial Naive Bayes, on the other hand, is used for classification problems where each feature represents the frequency of occurrence of a certain event. \n",
    "# For example, in a text classification problem, each feature can represent the frequency of a certain word in a document. \n",
    "# Multinomial Naive Bayes models the data as a set of discrete events, where each event represents the frequency of occurrence of a feature. \n",
    "# It assumes that the features are conditionally independent given the class label.\n",
    "\n",
    "# In summary, Bernoulli Naive Bayes is used for binary data while Multinomial Naive Bayes is used for discrete count data.\n",
    "# Bernoulli Naive Bayes models the data as binary events while Multinomial Naive Bayes models the data as discrete events representing \n",
    "# the frequency of occurrence of features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "9ab6f2b2-97fa-4f7d-b355-b609655e6f95",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q3. How does Bernoulli Naive Bayes handle missing values?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "4fbffbec-6198-49cc-806f-473e9353e80b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bernoulli Naive Bayes is a simple algorithm that assumes that each feature is conditionally independent given the class label. It is designed to handle binary data, \n",
    "# where each feature is either present or absent. In this case, missing values are treated as the absence of a feature.\n",
    "\n",
    "# When a feature is missing for a certain instance, Bernoulli Naive Bayes assumes that the feature is not present for that instance. \n",
    "# This can be problematic if there are a large number of missing values, as it can lead to bias in the estimation of the conditional probabilities.\n",
    "\n",
    "# One common approach to handle missing values in Bernoulli Naive Bayes is to impute them with a value that represents the absence of the feature. \n",
    "# This can be done by either replacing the missing values with zeros or with the mean or median of the non-missing values.\n",
    "\n",
    "# Another approach is to use a more sophisticated algorithm that can handle missing values, such as the Gaussian Naive Bayes or the Multinomial Naive Bayes. \n",
    "# Gaussian Naive Bayes can handle continuous data, and Multinomial Naive Bayes can handle discrete count data, both of which can accommodate missing values. \n",
    "# However, these algorithms may not be appropriate for binary data or may have other assumptions that are not appropriate for the specific problem at hand.\n",
    "\n",
    "# In summary, Bernoulli Naive Bayes treats missing values as the absence of a feature, but imputation techniques or alternative algorithms may be used \n",
    "# to handle missing values in a more appropriate manner."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "5b03f828-2617-4b03-80c2-d4b553e6bafc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q4. Can Gaussian Naive Bayes be used for multi-class classification?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "66760d37-5045-49a4-a2ca-8c1e0d01283a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Yes, Gaussian Naive Bayes can be used for multi-class classification problems.\n",
    "\n",
    "# In a binary classification problem, Gaussian Naive Bayes models each feature as a Gaussian distribution for each class,\n",
    "# where the mean and variance of the distribution are estimated from the data. The probability of an instance belonging to a certain class is \n",
    "# then calculated using Bayes' rule, which combines the prior probability of the class with the likelihood of the features given the class.\n",
    "\n",
    "# In a multi-class classification problem, Gaussian Naive Bayes extends this approach by modeling each feature as a Gaussian distribution for each class, \n",
    "# as in the binary case. However, instead of computing the probability of an instance belonging to a single class, \n",
    "# it computes the probability of the instance belonging to each class. This is done by applying Bayes' rule separately for each class and then normalizing \n",
    "# the resulting probabilities so that they sum up to 1.\n",
    "\n",
    "# There are other Naive Bayes algorithms that are specifically designed for multi-class classification, such as the Multinomial Naive Bayes and\n",
    "# the Complement Naive Bayes. However, Gaussian Naive Bayes can still be a good option for multi-class problems,\n",
    "# especially when the features are continuous and follow a Gaussian distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "6490511a-6181-46f8-bf91-6afb34973c48",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q5. Assignment:\n",
    "# Data preparation:\n",
    "\n",
    "# Download the \"Spambase Data Set\" from the UCI Machine Learning Repository (https://archive.ics.uci.edu/ml/\n",
    "# datasets/Spambase). This dataset contains email messages, where the goal is to predict whether a message \n",
    "# is spam or not based on several input features.\n",
    "\n",
    "\n",
    "# Implementation:\n",
    "\n",
    "# Implement Bernoulli Naive Bayes, Multinomial Naive Bayes, and Gaussian Naive Bayes classifiers using the \n",
    "# scikit-learn library in Python. Use 10-fold cross-validation to evaluate the performance of each classifier on the \n",
    "# dataset. You should use the default hyperparameters for each classifier.\n",
    "\n",
    "\n",
    "# Results:\n",
    "\n",
    "# Report the following performance metrics for each classifier:\n",
    "\n",
    "# Accuracy\n",
    "\n",
    "# Precision\n",
    "\n",
    "# Recall\n",
    "\n",
    "# F1 score\n",
    "\n",
    "\n",
    "# Discussion:\n",
    "\n",
    "# Discuss the results you obtained. Which variant of Naive Bayes performed the best? Why do you think that is \n",
    "# the case? Are there any limitations of Naive Bayes that you observed?\n",
    "\n",
    "\n",
    "# Conclusion:\n",
    "\n",
    "# Summarise your findings and provide some suggestions for future work\n",
    "\n",
    "# Note:  This dataset contains a binary classification problem with multiple features. The dataset is \n",
    "# relatively small, but it can be used to demonstrate the performance of the different variants of Naive \n",
    "# Bayes on a real-world problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "03a7a12c-1dea-448f-b646-3648fe0dcb07",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bernoulli Naive Bayes:\n",
      "  10-fold cross-validation accuracy: 0.8839 (std: 0.0467)\n",
      "  Overall accuracy: 0.8859\n",
      "  Precision: 0.8861\n",
      "  Recall: 0.8152\n",
      "  F1 score: 0.8492\n",
      "\n",
      "Multinomial Naive Bayes:\n",
      "  10-fold cross-validation accuracy: 0.7863 (std: 0.0400)\n",
      "  Overall accuracy: 0.7924\n",
      "  Precision: 0.7440\n",
      "  Recall: 0.7215\n",
      "  F1 score: 0.7326\n",
      "\n",
      "Gaussian Naive Bayes:\n",
      "  10-fold cross-validation accuracy: 0.8218 (std: 0.0772)\n",
      "  Overall accuracy: 0.8229\n",
      "  Precision: 0.7012\n",
      "  Recall: 0.9592\n",
      "  F1 score: 0.8102\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.naive_bayes import BernoulliNB, MultinomialNB, GaussianNB\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "import pandas as pd\n",
    "\n",
    "# Load data\n",
    "data = pd.read_csv('spambase.data', header=None)\n",
    "\n",
    "X = data.iloc[:, :-1]\n",
    "y = data.iloc[:, -1]\n",
    "\n",
    "# Define classifiers\n",
    "bnb = BernoulliNB()\n",
    "mnb = MultinomialNB()\n",
    "gnb = GaussianNB()\n",
    "\n",
    "# Evaluate classifiers using 10-fold cross-validation\n",
    "classifiers = [('Bernoulli Naive Bayes', bnb), ('Multinomial Naive Bayes', mnb), ('Gaussian Naive Bayes', gnb)]\n",
    "for name, clf in classifiers:\n",
    "    scores = cross_val_score(clf, X, y, cv=10)\n",
    "    accuracy = accuracy_score(y, clf.fit(X, y).predict(X))\n",
    "    precision = precision_score(y, clf.fit(X, y).predict(X))\n",
    "    recall = recall_score(y, clf.fit(X, y).predict(X))\n",
    "    f1 = f1_score(y, clf.fit(X, y).predict(X))\n",
    "    print(f\"{name}:\")\n",
    "    print(f\"  10-fold cross-validation accuracy: {scores.mean():.4f} (std: {scores.std():.4f})\")\n",
    "    print(f\"  Overall accuracy: {accuracy:.4f}\")\n",
    "    print(f\"  Precision: {precision:.4f}\")\n",
    "    print(f\"  Recall: {recall:.4f}\")\n",
    "    print(f\"  F1 score: {f1:.4f}\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf89c5d8-8b5a-4e97-88da-870dc770222b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
